# Mixture of Experts

传统的模型称之为稠密模型，将同一个套参数用在所有token上。若想提升容量，通常要扩大网络宽度或深度，导致每个token的计算增加。同时，稠密模型中还有一个问题：虽然Transformer构建了庞大的参数网络，但是其某些层可能非常稀疏，某些神经元的激活频率会低于其他神经元。

MoE的核心想法是：参数容量随专家增加，但每个token只激活少数专家（稀疏激活），从而实现了参数量大，计算量接近小模型，一定条件下模型表达力能接近大模型。

