# Optimizing a CUDA Softmax Kernel: A Deep Dive into Reductions, Memory, and Warp-Level Primitives

æœ¬æ–‡è®²è§£å¦‚ä½•ä¸€æ­¥ä¸€æ­¥ç”¨CUDAå®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„Softmaxï¼ˆå•ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼‰ç®—å­ã€‚

## Softmax ç®—æ³•

Softmax æŠŠä¸€ç»„ä»»æ„å®æ•°ï¼Œå˜æˆã€å¯æ¯”è¾ƒçš„æ¦‚ç‡åˆ†å¸ƒã€ã€‚

åšäº†ä¸‰ä»¶äº‹ï¼š

1. æ¯ä¸ªå€¼éƒ½å˜æˆ **éè´Ÿ**
2. æ‰€æœ‰å€¼ **å’Œä¸º 1**
3. å¤§çš„å€¼ä¼šè¢« **æŒ‡æ•°çº§æ”¾å¤§**ï¼Œå°çš„å€¼è¢«å‹ç¼©

ç»™å®šä¸€ä¸ªå‘é‡ï¼š
```math
\mathbf{z} = (z_1, z_2, \dots, z_n)
```
Softmax å®šä¹‰ä¸ºï¼š
```math
\text{Softmax}(z_i)
= \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
```
è¾“å‡ºï¼š
```math
\mathbf{p} = (p_1, p_2, \dots, p_n)
```

ä¸ºä»€ä¹ˆç”¨æŒ‡æ•°å‡½æ•°ï¼Ÿå› ä¸ºï¼š

- ä¿è¯æ­£æ•°

$$
e^{z_i} > 0
$$

- æ”¾å¤§å·®å¼‚ï¼Œsoftmax ä¼šè®©ã€æœ€å¤§ logitã€ä¸»å¯¼åˆ†å¸ƒï¼š

  - å·® 1 â†’ æ¯”ä¾‹ â‰ˆ 2.7 å€

  - å·® 5 â†’ æ¯”ä¾‹ â‰ˆ 148 å€

- å¯å¾®ï¼Œæ„å‘³ç€å¯ä»¥åå‘ä¼ æ’­ï¼Œé€‚åˆç”¨åœ¨ç¥ç»ç½‘ç»œä¸­

 Softmax çš„æ ¸å¿ƒæ€§è´¨ï¼š

- è¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒï¼š

$$
0 < p_i < 1,\quad \sum_i p_i = 1
$$

- å¹³ç§»ä¸å˜æ€§ï¼ˆæ•°å€¼ç¨³å®šå…³é”®ï¼‰ï¼š

$$
\text{Softmax}(z)=\text{Softmax}(z - c)
$$

- ä¸å…·å¤‡å°ºåº¦ä¸å˜æ€§ï¼š
  $$
  \text{Softmax}(\alpha z) \neq \text{Softmax}(z)
  $$

è¦æ³¨æ„çš„æ˜¯ï¼ŒSoftmaxä¸­çš„æŒ‡æ•°å‡½æ•°å¢é•¿æ˜¯éå¸¸å¿«çš„ï¼š

- $e^{10} \approx 2.2 \times 10^4$
- $e^{50} \approx 5.2 \times 10^{21}$
- $e^{100} \approx 2.7 \times 10^{43}$

ä½† **float32** èƒ½è¡¨ç¤ºçš„æœ€å¤§å€¼å¤§çº¦æ˜¯ï¼š

$$
\text{max float32} \approx 3.4 \times 10^{38}
$$

ä¹Ÿå°±è¯´ï¼Œä¸€æ—¦ $z_i \gtrsim 88$ ï¼Œ $e^{z_i}$ **ç›´æ¥æº¢å‡ºæˆ `Inf`**ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸å¯æ¥å—çš„ï¼Œäºæ˜¯åˆ©ç”¨å¹³ç§»ä¸å˜æ€§ï¼Œæˆ‘ä»¬åƒå¦‚ä¸‹æ”¹é€ å…¬å¼ï¼š

$$
\text{Softmax}(z_i)
= \frac{e^{z_i - \max(z)}}{\sum_j e^{z_j - \max(z)}}
$$

æ˜¾ç„¶æœ‰ï¼š

$$
e^{z_i - \max(z)} \le 1
$$

å› æ­¤ï¼Œä¸å¯èƒ½å†æº¢å‡ºã€‚

Softmaxåœ¨LLMä¸­æœ‰éå¸¸é‡è¦çš„ç”¨é€”ï¼Œä¸»è¦ä½“ç°åœ¨Attention æƒé‡å½’ä¸€åŒ–ã€‚

åœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œæœ‰å¦‚ä¸‹å…¬å¼ï¼š

$$
A = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)
$$

æŠŠæ¯ä¸ª Query å¯¹æ‰€æœ‰ Key çš„ç›¸ä¼¼åº¦ï¼Œè½¬æ¢æˆ **ã€æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒã€**ï¼Œæ¯ä¸€è¡Œè¡¨ç¤º"æˆ‘è¯¥å…³æ³¨è°"ï¼Œå¼ºè°ƒæœ€ç›¸å…³ tokenã€‚

## V1 æœ€æœ´ç´ Softmaxè®¡ç®—

å¯ä»¥å¾ˆè½»æ¾åœ°å®ç°ä¸€ä¸ªæœ€æœ´ç´ ç‰ˆæœ¬çš„Softmax Kernelï¼š

```c
// CUDA kernel for computing the softmax function.
// Each thread processes one row of the input matrix.
__global__ void softmax_forward_kernel_v1(float *__restrict__ output,      // [N, C] output tensor
                                          const float *__restrict__ input, // [N, C] input tensor
                                          int num_rows,                    // N: number of rows
                                          int num_cols                     // C: number of columns per row
) {
    // Global thread index corresponding to the row index
    int row_idx = blockIdx.x * blockDim.x + threadIdx.x;

    // Guard against out-of-bounds threads
    if (row_idx >= num_rows) {
        return;
    }

    // Pointers to the current row
    const float *input_row = input + row_idx * num_cols;
    float *output_row = output + row_idx * num_cols;

    // Step 1: find the maximum value in the row (for numerical stability)
    float max_value = -CUDART_INF_F;
    for (int col = 0; col < num_cols; ++col) { max_value = fmaxf(max_value, input_row[col]); }

    // Step 2: compute exponentials and their sum
    float sum_exp = 0.0f;
    for (int col = 0; col < num_cols; ++col) {
        float exp_val = expf(input_row[col] - max_value);
        output_row[col] = exp_val;
        sum_exp += exp_val;
    }

    // Step 3: normalize to obtain probabilities
    float inv_sum = 1.0f / sum_exp;
    for (int col = 0; col < num_cols; ++col) { output_row[col] *= inv_sum; }
}
```

æˆ‘ä»¬è®©æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€æ•´è¡Œï¼Œéå†è¡Œä¸­æ‰€æœ‰å…ƒç´ ï¼Œæ‰¾åˆ°è¡Œä¸­çš„æœ€å¤§å€¼ï¼Œæ¥ç€æ±‚å’Œç®—å‡ºåˆ†æ¯ï¼Œä»£å…¥è®¡ç®—åå¾—åˆ°æœ€ç»ˆç»“æœã€‚

ç”¨Nsight Computeåˆ†æä¸€ä¸‹è¿™ä¸ªkernelçš„æ€§èƒ½ï¼š

![softmax_v1_1](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_1.png)

å¯ä»¥çœ‹åˆ°ï¼ŒNsightè¡¨æ˜ï¼š"**This kernel grid is too small to fill the available resources on this device, resulting in only 0.06 full waves across all SMs.**"ï¼ŒGPU Speed Of Light Throughputä¸­çš„æŒ‡æ ‡æ•°æ®ä¹Ÿä½“ç°äº†è¿™ä¸€ç‚¹ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„GPUä¸­çš„SMå¾ˆç©ºé—²ï¼Œå¤§é‡çš„ç®—åŠ›éƒ½æ²¡ç”¨ä¸Šï¼ŒGPU çš„ç†è®ºå³°å€¼å®Œå…¨ç”¨ä¸ä¸Šã€‚

è¿™é‡Œè¯´"Small Grid"ï¼Œæœ¬è´¨ä¸Šæ˜¯å› ä¸ºå¹¶è¡Œæ˜ å°„æ–¹å¼çš„é—®é¢˜ã€‚æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è®¾å®šN=C=4096ï¼Œgrid.x è®¡ç®—æ–¹å¼æ˜¯ï¼š

```c
grid.x = ceil(N / blockDim.x)
```

åŒæ—¶è®¾ç½®ï¼š

```c
N = 4096
blockDim.x = 128
```

äºæ˜¯å¾—åˆ°ï¼šgrid.x = 4096 / 128 = 32 blocks

Nsightåˆ¤æ–­æ˜¯å¦æ˜¯Small Gridï¼Œæ˜¯çœ‹ï¼šèƒ½ä¸èƒ½åœ¨æ‰€æœ‰ SM ä¸Šå½¢æˆè¶³å¤Ÿå¤šçš„ "waves"ï¼ŒwaveæŒ‡çš„æ˜¯ï¼š

- **1 wave** â‰ˆ *æ‰€æœ‰ SM åŒæ—¶è‡³å°‘è·‘ 1 ä¸ª block*
- **full wave** â‰ˆ GPU çš„æ¯ä¸ª SM éƒ½åœ¨å¹²æ´»

0.06 full waves å°±è¡¨æ˜äº†å¤§é‡SMç©ºé—²ã€‚

å†ä»ä»£ç è§’åº¦åˆ†æï¼Œè¿™ä¸ªkernelä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢ã€‚

é¦–å…ˆæ˜¯è¿™æ®µä»£ç ï¼š

```c
// Step 1: find the maximum value in the row (for numerical stability)
float max_value = -CUDART_INF_F;
for (int col = 0; col < num_cols; ++col) { max_value = fmaxf(max_value, input_row[col]); }
```

è¿™æ®µä»£ç æ˜¯é¡ºåºéå†ä¸€æ•´è¡Œï¼Œæ‰¾åˆ°æœ€å¤§å€¼ï¼Œå®é™…ä¸Šè¿™ä¸€æ­¥éª¤å¯ä»¥å¹¶è¡Œæ‰§è¡Œã€‚

æ±‚å’Œä¹Ÿæ˜¯ä¸€æ ·ï¼š

```c
// Step 2: compute exponentials and their sum
float sum_exp = 0.0f;
for (int col = 0; col < num_cols; ++col) {
    float exp_val = expf(input_row[col] - max_value);
    output_row[col] = exp_val;
    sum_exp += exp_val;
}
```

åŒæ ·æ˜¯éå†ï¼Œè¿™éƒ¨åˆ†ä¹Ÿå¯ä»¥å¹¶è¡ŒåŒ–ã€‚

## V2 Shared Memory & Block Reduce

ä¸ºäº†ä¼˜åŒ–ä¸Šè¿°ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬åŸºäºå…±äº«å†…å­˜å®ç°å—å†…åŠ é€Ÿã€‚

å¯ä»¥è¿™æ ·è®¡ç®—è®¡ç®—æœ€å¤§å€¼ï¼Œæ¯ä¸ªçº¿ç¨‹å…ˆè®¡ç®—å±€éƒ¨æœ€å¤§å€¼ï¼š

```c
float local_max = -CUDART_INF_F;
for (int col = tid; col < num_cols; col += block_size) {
    local_max = fmaxf(local_max, input_row[col]);
}
```

æ¯ä¸ªçº¿ç¨‹å®é™…ä¸Šå¤„ç†ï¼š

```
thread 0  â†’ columns: 0, block_size, 2*block_size, ...
thread 1  â†’ columns: 1, block_size+1, 2*block_size+1, ...
...
```

æ¯ä¸ªçº¿ç¨‹éš”ä¸€ä¸ª block_size å–ä¸€ä¸ªå…ƒç´ ï¼Œä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªå…ƒç´ ï¼Œå«ä½œçº¿ç¨‹ç²—åŒ–ï¼ˆthread coarseningï¼‰ã€‚local_maxæ˜¯æ¯ä¸ªçº¿ç¨‹è®¡ç®—å‡ºçš„å±€éƒ¨æœ€å¤§å€¼ã€‚

æŠŠå±€éƒ¨æœ€å¤§å€¼å†™å…¥ shared memoryï¼š

```c
shared[tid] = local_max;
```

è®¡ç®—å®Œå±€éƒ¨æœ€å¤§å€¼åï¼Œå°±å¼€å§‹å¹¶è¡Œè§„çº¦ï¼š

```c
for (int stride = block_size / 2; stride > 0; stride >>= 1) {
    if (tid < stride) {
        shared[tid] = fmaxf(shared[tid], shared[tid + stride]);
    }
    __syncthreads();
}
```

æ¯ä¸€è½®è§„çº¦ï¼Œå‚ä¸çš„çº¿ç¨‹æ•°é‡å‡åŠï¼Œè¿™æ˜¯ä¸€ä¸ª**æ ‘å½¢å¹¶è¡Œè§„çº¦ï¼ˆtree reductionï¼‰**ï¼š

- ç¬¬ 1 è½®ï¼š
  - 128 â†’ 64 ä¸ªå€¼
- ç¬¬ 2 è½®ï¼š
  - 64 â†’ 32
- â€¦
- æœ€åï¼š
  - 1 ä¸ªå€¼ï¼ˆshared[0]ï¼‰

æœ€ç»ˆå¾—åˆ°ï¼š

```c
shared[0] = max(input_row[0..num_cols-1])
```

æ±‚å’Œçš„åšæ³•æ˜¯ç›¸è¿‘çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```c
// Broadcast the maximum value to all threads
float max_value = shared[0];

// ------------------------------------------------------------------
// Step 2: compute exponentials and their sum
// Each thread again processes multiple columns
// ------------------------------------------------------------------
float local_sum = 0.0f;
for (int col = tid; col < num_cols; col += block_size) {
    float exp_val = expf(input_row[col] - max_value);
    output_row[col] = exp_val;
    local_sum += exp_val;
}

// Write partial sums to shared memory
shared[tid] = local_sum;
__syncthreads();

// Block-level reduction to compute the sum of exponentials
for (int stride = block_size / 2; stride > 0; stride >>= 1) {
    if (tid < stride) {
        shared[tid] += shared[tid + stride];
    }
    __syncthreads();
}

// Broadcast the sum to all threads
float sum_exp = shared[0];
```

è®¡ç®—æµç¨‹å¤§è‡´å¦‚ä¸‹ï¼š

![softmax_v2_1](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_3.png)

ä½¿ç”¨Nsightåˆ†æè¯¥kernelï¼š

![softmax_v2_2](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_4.png)

å¯ä»¥çœ‹åˆ°æ€§èƒ½æå‡äº†ç›¸å½“å¤šï¼ŒSmall Gridçš„é—®é¢˜ä¹Ÿæ¶ˆå¤±äº†ï¼Œå¹¶è¡Œåº¦å¤§å¤§æé«˜ï¼Œä¸»è¦é—®é¢˜æ¥åˆ°äº† **å†…å­˜å­ç³»ç»Ÿï¼ˆå°¤å…¶æ˜¯ DRAMï¼‰**ï¼Œé‚£ä¹ˆä¸‹é¢å°±ç€æ‰‹é’ˆå¯¹è¿™ä¸€ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚

æ³¨æ„åˆ°æœ‰ä¸€æ ï¼š

![softmax_v2_3](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_6.png)

è¿™ä¸ªæ•°æ®æ­ç¤ºäº†"åˆ°åº•æ˜¯å“ªå‡ æ¡æŒ‡ä»¤ï¼Œåœ¨ warp stall ä¸­è´¡çŒ®æœ€å¤§ï¼Ÿ"ï¼Œç‚¹å‡»ç¬¬ä¸€æ¡è¢«è®¤ä¸ºæ˜¯æœ€å¤šè´¡çŒ®çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬æ¥åˆ°æ–°çš„é¡µé¢ã€‚

é€šè¿‡è¿™äº›æ›´åº•å±‚çš„æ•°æ®ï¼Œå¯ä»¥ä»**GPU æ‰§è¡Œå±‚ï¼ˆSASSï¼‰+ å¾®æ¶æ„è°ƒåº¦å±‚**çœ‹åˆ°æ›´ä¸ºç»†è‡´çš„æ•°æ®ï¼š

![softmax_v2_4](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_5.png)

è¿™å¼ å›¾çš„æœ€å·¦ä¾§ä¸€æ å±•ç°çš„æ˜¯SASS æŒ‡ä»¤ï¼š

```
IMAD.WIDE.U32
LDG.E.CONSTANT
FFMA / FMNMX
BRA
```

å¯ä»¥ç®€å•äº†è§£ä¸€äº›æŒ‡ä»¤ï¼š

| æŒ‡ä»¤               | å«ä¹‰                                        |
| ------------------ | ------------------------------------------- |
| `IMAD.WIDE.U32`    | 32-bit integer multiply-addï¼ˆåœ°å€è®¡ç®—å¸¸è§ï¼‰ |
| `LDG.E.CONSTANT`   | ä» constant / global memory è¯»              |
| `FMNMX`            | æµ®ç‚¹ min/maxï¼ˆsoftmax çš„ max reductionï¼‰    |
| `FFMA / FADD`      | æµ®ç‚¹è¿ç®—                                    |
| `BRA`              | åˆ†æ”¯                                        |
| `BSYNC.RECONVERGE` | warp åˆ†æ”¯é‡æ±‡åˆ                             |

ä¸­é—´è“è‰²æ¡å±•ç°çš„æ˜¯æ¯ä¸ªçº¿ç¨‹ï¼ˆthreadï¼‰æ­£åœ¨å ç”¨çš„å¯„å­˜å™¨æ•°é‡ï¼Œè¿™æ¡æŒ‡ä»¤æ‰§è¡Œæ—¶ï¼Œæ´»è·ƒçš„å¯„å­˜å™¨æ•°é‡ã€‚

å³ä¾§çš„ä¸¤åˆ—ï¼Œ**`Attributed Stalls`** å’Œ **`Warp Stall Sampling (Not-issued Samples)`** è¡¨ç¤ºï¼š

|                     æŒ‡æ ‡                     |        æ ¸å¿ƒé—®é¢˜         |                           å®šä¹‰                            |
| :------------------------------------------: | :---------------------: | :-------------------------------------------------------: |
| **Warp Stall Sampling (Not-issued Samples)** | *warp ä»€ä¹ˆæ—¶å€™æ²¡å‘å°„ï¼Ÿ* | ç»Ÿè®¡warp æœ¬åº”å‘å°„æŒ‡ä»¤ä½†æ²¡æœ‰å‘å°„çš„é‡‡æ ·æ¬¡æ•°ï¼Œä¹Ÿå°±æ˜¯åœé¡¿æ¬¡æ•° |
|            **Attributed Stalls**             |    *ä¸ºä»€ä¹ˆæ²¡å‘å°„ï¼Ÿ*     |     å°è¯•åˆ¤æ–­åŸå› ï¼ŒæŠŠæ²¡å‘å°„çš„ stall æŒ‰åŸå› å½’å› åçš„ç»“æœ     |

å¯ä»¥å‘ç°ï¼Œæœ‰äº›æŒ‡ä»¤çš„Attributed Warp Stall Samplingæ˜¯ç©ºçš„ï¼Œå› ä¸ºè¿™äº›æŒ‡ä»¤åœ¨é‡‡æ ·æ—¶åˆ»æ²¡æœ‰ã€Attributed Stallã€ï¼Œè¿™äº›æŒ‡ä»¤è¦ä¹ˆæ²¡æœ‰å¯¼è‡´ warp stallï¼Œ è¦ä¹ˆ stall å·²ç»è¢«å½’å› ç»™äº†åˆ«çš„æŒ‡ä»¤ã€‚è€ƒè™‘Nsightçš„æµ‹è¯•æœºåˆ¶ï¼ŒNsight çš„ stall sampling åªåœ¨ warp æ²¡èƒ½å‘å°„æŒ‡ä»¤æ—¶æ‰è¿›è¡Œé‡‡æ ·ï¼Œåªç»™ã€å¯¼è‡´ stall çš„é‚£æ¡æŒ‡ä»¤ã€å½’å› ï¼Œæ‰€ä»¥ä¸æ˜¯æ¯æ¡æŒ‡ä»¤éƒ½ä¼šè¢«é‡‡æ ·ï¼Œä¸æ˜¯æ¯æ¡æŒ‡ä»¤éƒ½ä¼šæœ‰ stallã€‚

ä¸€æ¡ SASS æŒ‡ä»¤**åªæœ‰åœ¨åŒæ—¶æ»¡è¶³ä¸‹é¢æ¡ä»¶æ—¶**ï¼Œæ‰å¯èƒ½æ˜¾ç¤º stall ç™¾åˆ†æ¯”ï¼š

- warp æƒ³æ‰§è¡Œä¸‹ä¸€æ¡æŒ‡ä»¤ï¼Œä½†ä¸èƒ½æ‰§è¡Œ

- åŸå› èƒ½æ˜ç¡®å½’å› åˆ°æŸä¸€ç±»ï¼ˆFP / memory / barrier / scoreboard ç­‰ï¼‰

- é‡‡æ ·ç‚¹æ­£å¥½è½åœ¨è¿™ä¸ªç­‰å¾…é˜¶æ®µ

çº¯ç®—æœ¯ã€æ— ä¾èµ–çš„æŒ‡ä»¤ä¸€èˆ¬æ²¡æœ‰Attributed Stallï¼Œå› ä¸ºå»¶è¿Ÿéå¸¸ä½ï¼Œwarp æ‰§è¡Œå®Œåï¼Œä¸‹ä¸€æ¡æŒ‡ä»¤ç«‹åˆ» readyã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯å‚è€ƒï¼šhttps://docs.nvidia.com/nsight-compute/ProfilingGuide/

æˆ‘ä»¬ç‚¹å‡»å¯¹Warp Stallè´¡çŒ®æœ€å¤šçš„æŒ‡ä»¤ï¼Œå¯ä»¥çœ‹åˆ°ï¼š

![softmax_v2_5](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_7.png)

Scoreboard Stalls = **44.6% (7.18K)**ï¼Œè¡¨ç¤ºåœ¨æ‰€æœ‰ç”±äºscoreboardï¼ˆå¯„å­˜å™¨ä¾èµ–ï¼‰è€Œäº§ç”Ÿçš„ stall é‡‡æ ·ä¸­ï¼Œæœ‰ 44.6% çš„ stall æ˜¯ç”± Floating Point æŒ‡ä»¤å¼•èµ·çš„ï¼Œå¯¹åº”çº¦ 7,180 æ¬¡é‡‡æ ·ï¼Œä¹Ÿå°±è¯´ï¼š**warp å¡ä½çš„æ—¶å€™ï¼Œå°†è¿‘ä¸€åŠæ˜¯åœ¨ç­‰æµ®ç‚¹æŒ‡ä»¤çš„ç»“æœå†™å›å¯„å­˜å™¨ã€‚**

Nsightåœ¨è¿™é‡Œä¼šæŒ‡ç¤ºscoreboard ç­‰å¾…çš„æœ€åä¸€ä¸ªå†™å¯„å­˜å™¨çš„æŒ‡ä»¤æ˜¯ä»€ä¹ˆç±»å‹ï¼Œæ‰€ä»¥æŒ‡å‘äº†ç¬¬70è¡Œï¼Œä½œä¸ºã€è¾“å…¥ä¾èµ–æºã€ï¼š

```
LDG.E.CONSTANT R12, desc[UR8][R12.64]					
```

ç¬¬ 70 è¡Œè¢«æ ‡è®°ï¼Œæ˜¯å› ä¸ºå®ƒæ˜¯ scoreboard ç­‰å¾…å¯„å­˜å™¨çš„æœ€è¿‘å†™å…¥è€…ï¼Œä½†æ˜¯è¿™å¹¶ä¸è¡¨æ˜loadæ˜¯æ€§èƒ½ç“¶é¢ˆçš„"ç½ªé­ç¥¸é¦–"ï¼Œå› ä¸ºè¿™æ¡æŒ‡ä»¤çš„Attributed Warp Stall Samplingå¾ˆä½ï¼Œä¸»è¦åŸå› è¿˜æ˜¯FPä¾èµ–ï¼Œè¿™ä»ä»£ç é€»è¾‘ä¸­å°±å¯ä»¥æ¨æ–­å‡ºæ¥ã€‚

ç»“åˆä»£ç ï¼Œè¿™äº›æŒ‡ä»¤å®é™…ä¸Šå‘ç”Ÿåœ¨ï¼š

```c
// Write partial maxima to shared memory
shared[tid] = local_max;
__syncthreads();

// Block-level reduction to find the maximum value
for (int stride = block_size / 2; stride > 0; stride >>= 1) {
    if (tid < stride) {
        shared[tid] = fmaxf(shared[tid], shared[tid + stride]);
    }

    __syncthreads();
}
```

å…¶ç‰¹æ€§æ˜¯ï¼š

- ä¸‹ä¸€æ¬¡è¿­ä»£ **å¿…é¡»ç­‰å¾…** ä¸Šä¸€æ¬¡ç»“æœ
- warp å†…æ— æ³•å¹¶è¡Œ
- scheduler æ— æ³•æ‰“æ•£

è¿™é‡Œçš„å¾ªç¯æºå¸¦ä¾èµ–æ‰æ˜¯FMNMXè¢«åœé¡¿çš„æ ¹æœ¬åŸå› ï¼Œå¯¼è‡´äº†å¤§é‡çš„FPä¾èµ–ã€‚è™½ç„¶å®šä½åˆ°äº†é—®é¢˜ï¼Œä½†æ˜¯å¾ˆé—æ†¾ï¼Œç¬”è€…æ²¡æœ‰èƒ½åŠ›ä¼˜åŒ–è¿™ä¸ªç“¶é¢ˆï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„äºŒå‰æ ‘è§„çº¦ï¼Œè¿™ä¸€å±‚çš„æ•°æ®è¦ä¾èµ–ä¸Šä¸€å±‚çš„æ•°æ®è¿™é¡ºç†æˆç« ï¼Œç¬”è€…å¹¶æ— å…ˆéªŒçŸ¥è¯†å¯ä»¥æ¶ˆé™¤è¿™ä¸ªä¾èµ–ğŸ¥¹

## V3 Warp Shuffle Instructions

ä¸è¿‡å¥½åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è®¿å­˜å†åšä¸€æ­¥ä¼˜åŒ–ã€‚

åœ¨è¿™ä¸€ç‰ˆæœ¬ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ç§ç‰¹åˆ«çš„æŒ‡ä»¤ï¼šæ´—ç‰ŒæŒ‡ä»¤ï¼Œå³shuffleæŒ‡ä»¤ã€‚

æ´—ç‰ŒæŒ‡ä»¤ä¸€èˆ¬å½¢å¼å¦‚ä¸‹ï¼š

```c
__shfl_xxx_sync(mask, value, src_lane, width)
```

å‚æ•°å¦‚ä¸‹ï¼š

|   å±æ€§   |          è¯´æ˜          |
| :------: | :--------------------: |
|   èŒƒå›´   |     ä»…é™ä¸€ä¸ª warp      |
| é€šä¿¡ä»‹è´¨ |         å¯„å­˜å™¨         |
|   åŒæ­¥   |    warp å†…éšå¼åŒæ­¥     |
|   å»¶è¿Ÿ   |         éå¸¸ä½         |
| é€‚ç”¨åœºæ™¯ | å½’çº¦ã€æ‰«æã€å¹¿æ’­ã€é‡æ’ |

æˆ‘ä»¬çŸ¥é“ï¼ŒWarp æ‰§è¡Œæ¨¡å‹æœ‰ï¼š

- ä¸€ä¸ª warp = 32 ä¸ªçº¿ç¨‹
- warp å†… **SIMT é”æ­¥æ‰§è¡Œ**
- æ¯ä¸ªçº¿ç¨‹æœ‰è‡ªå·±çš„å¯„å­˜å™¨

æ´—ç‰ŒæŒ‡ä»¤**æ‰“ç ´äº†ã€çº¿ç¨‹åªèƒ½è®¿é—®è‡ªå·±å¯„å­˜å™¨ã€çš„é™åˆ¶**ï¼Œä½†åªåœ¨ warp å†…æœ‰æ•ˆã€‚

 `__shfl_sync`ï¼ˆä»»æ„ lane è®¿é—®ï¼‰

```c
int v = __shfl_sync(0xffffffff, x, srcLane);
```

æ‰€æœ‰çº¿ç¨‹ä» `srcLane` çº¿ç¨‹è¯»å– `x`ï¼Œç”¨äºå¹¿æ’­ / æ”¶é›†æ•°æ®éå¸¸é«˜æ•ˆ

`__shfl_up_sync`ï¼ˆå‘ä¸Šç§»åŠ¨ï¼‰

```c
int v = __shfl_up_sync(mask, x, delta);
```

ä» `laneId - delta` è¯»å–ï¼Œå¦‚æœlaneId < deltaï¼Œå‡½æ•°ä¼šè¿”å›æœªå®šä¹‰

`__shfl_down_sync`ï¼ˆå‘ä¸‹ç§»åŠ¨ï¼‰

```c
int v = __shfl_down_sync(mask, x, delta);
```

ä» `laneId + delta` è¯»å–ï¼Œå¸¸ç”¨äºå½’çº¦ï¼ˆreduceï¼‰

 `__shfl_xor_sync`ï¼ˆè¶å½¢äº¤æ¢ï¼‰

```c
int v = __shfl_xor_sync(mask, x, laneMask);
```

`srcLane = laneId ^ laneMask`ï¼Œéå¸¸é€‚åˆ **æ ‘å½¢ / butterfly ç»“æ„**

è¿™ç±»æŒ‡ä»¤éå¸¸ç”¨äºå—å†…è§„çº¦ï¼Œä¸€ä¸ªblock å†…å…ˆç”¨ shuffleï¼Œå†ç”¨ shared memory åˆå¹¶ warpï¼š

```c
__device__ __forceinline__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) {
        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}

__device__ __forceinline__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}
```

ä»¥æ±‚å’Œä¸ºä¾‹ï¼Œ**warp å†…çš„æ¯ä¸ªçº¿ç¨‹**ä» **lane_id + offset** çš„çº¿ç¨‹é‚£é‡Œï¼Œè¯»å–å®ƒçš„ `val` å¯„å­˜å™¨å€¼ï¼Œé‡åˆ°è¶Šç•Œçš„è¯»å–ç›´æ¥è¿”å›åŸå€¼ã€‚

| å½“å‰çº¿ç¨‹ lane_id |    è¯»å–è°çš„ val    |
| :--------------: | :----------------: |
|        0         |       lane 1       |
|        1         |       lane 2       |
|        â€¦         |         â€¦          |
|        30        |      lane 31       |
|        31        | ï¼ˆè¶Šç•Œï¼Œå€¼æœªå®šä¹‰ï¼‰ |

è¿™ä¸€æ“ä½œä¸­ï¼š

- æ²¡æœ‰ä»»ä½•å†…å­˜è®¿é—®
- æ²¡æœ‰ shared memory
- æ²¡æœ‰åŒæ­¥

ç”±æ­¤å®ç°äº†ä¸€ä¸ª**æ ‡å‡†çš„ warp çº§äºŒå‰æ ‘è§„çº¦ï¼ˆtree reductionï¼‰**

 ç¬¬ 1 è½®ï¼šoffset = 16

```c
val += value from lane_id + 16
```

ç»“æœï¼š

- lane 0 å¾—åˆ°ï¼š`v0 + v16`
- lane 1 å¾—åˆ°ï¼š`v1 + v17`
- â€¦
- lane 15 å¾—åˆ°ï¼š`v15 + v31`
- lane 16â€“31ï¼šç»“æœæ— æ„ä¹‰ï¼ˆåé¢ä¸ä¼šå†ç”¨ï¼‰

ä»32ä¸ªå…ƒç´ ä¸­å¾—åˆ°16 ä¸ªæœ‰æ•ˆ partial sumï¼Œ

ç¬¬ 2 è½®ï¼šoffset = 8

```c
val += value from lane_id + 8
```

- lane 0ï¼š`(v0+v16) + (v8+v24)`
- lane 1ï¼š`(v1+v17) + (v9+v25)`
- â€¦

ä»16ä¸ªå…ƒç´ å¾—åˆ°8ä¸ªå’Œï¼Œæ¥ç€ä»¥æ­¤ç±»æ¨ã€‚

å°†æ–°çš„Reduceæ›¿æ¢åˆ°åŸæ¥çš„ä»£ç ä¸­ï¼š

```c
// ------------------------------------------------------------------
// Shared memory layout:
// [0 ... warps_per_block - 1]          -> warp max values
// [warps_per_block ... 2*warps_per_block - 1] -> warp sum values
// ------------------------------------------------------------------
extern __shared__ float shared[];
float *warp_max = shared;
float *warp_sum = shared + warps_per_block;

// ------------------------------------------------------------------
// Step 1: compute maximum value of the row (numerical stability)
// Thread coarsening + warp-level reduction
// ------------------------------------------------------------------
float local_max = -CUDART_INF_F;
for (int col = tid; col < num_cols; col += block_size) {
    local_max = fmaxf(local_max, input_row[col]);
}

// Warp-level max reduction
local_max = warpReduceMax(local_max);

// Write warp result to shared memory
if (lane_id == 0) {
    warp_max[warp_id] = local_max;
}
__syncthreads();

// Block-level reduction across warps
if (tid == 0) {
    float max_val = warp_max[0];
    for (int i = 1; i < warps_per_block; ++i) {
        max_val = fmaxf(max_val, warp_max[i]);
    }
    warp_max[0] = max_val;
}
__syncthreads();

float max_value = warp_max[0];

// ------------------------------------------------------------------
// Step 2: compute exponentials and their sum
// Thread coarsening + warp-level reduction
// ------------------------------------------------------------------
float local_sum = 0.0f;
for (int col = tid; col < num_cols; col += block_size) {
    float exp_val = expf(input_row[col] - max_value);
    output_row[col] = exp_val;
    local_sum += exp_val;
}

// Warp-level sum reduction
local_sum = warpReduceSum(local_sum);

// Write warp sum to shared memory
if (lane_id == 0) {
    warp_sum[warp_id] = local_sum;
}
__syncthreads();

// Block-level reduction across warps
if (tid == 0) {
    float sum_val = warp_sum[0];
    for (int i = 1; i < warps_per_block; ++i) {
        sum_val += warp_sum[i];
    }
    warp_sum[0] = sum_val;
}
__syncthreads();

float sum_exp = warp_sum[0];
```

å–ç¼”äº†åŸæ¥å¯¹å…±äº«å†…å­˜çš„é¢‘ç¹ä½¿ç”¨ã€‚

ä½†æ˜¯å‡ºäººæ„æ–™çš„æ˜¯ï¼Œåœ¨åˆæ­¥çš„æµ‹è¯•ä¸­ï¼ŒV3å’ŒV2çš„æ‰§è¡Œæ—¶é—´å±…ç„¶å‡ ä¹ä¸€æ ·ï¼ğŸ¤¨

ä½†æ˜¯æˆ‘ä»¬å¦‚æœè°ƒå¤§V2å’ŒV3çš„`block_size`ï¼Œä»128è®¾ç½®åˆ°1024ï¼Œå´å¯ä»¥å‘ç°V3çš„æ€§èƒ½ç›¸è¾ƒV2å¤§å¤§æé«˜ã€‚

![softmax_v3_1](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_8.png)

![softmax_v3_2](https://github.com/T4t4KAU/ai_infra_blogs/blob/main/cuda_programming/img/softmax_9.png)

é‚£æ˜¯å› ä¸ºblock_sizeå¾ˆå¤§æ—¶ï¼Œåˆ™Reduceçš„è½®æ•°å¤§å¤§å¢é«˜ï¼ŒV2çš„åŒæ­¥å¼€é”€æ‰æš´éœ²å‡ºæ¥ï¼Œè¿™æ ·ä¸€æ¥æ‰ä½“ç°å‡ºV3çš„ä¼˜åŠ¿ã€‚

